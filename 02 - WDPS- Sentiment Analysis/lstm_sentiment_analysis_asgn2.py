# -*- coding: utf-8 -*-
"""LSTM Sentiment Analysis_asgn2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wuH1DFuawf8AGpGX4IA5l3U7N8BBx6il

# Part 0 - Data Preparation
"""

### Part 0 - Data Preperation 
import pandas as pd
path1="/content/drive/MyDrive/Colab Notebooks/01 EDS_Assignments/P202 WDPS/asgn 02/Racism_Dataset/train.csv"
df= pd.read_csv(path1, sep=',')
df.head(10)

tweet_df = df[['tweet','label']]

tweet_df_copy = tweet_df.copy()
tweet_train_set = tweet_df_copy.sample(frac=0.75, random_state=0)
tweet_test_set = tweet_df_copy.drop(tweet_train_set.index)

print(tweet_df.shape)
print(tweet_train_set.shape)
print(tweet_test_set.shape)

sentiment_label = tweet_train_set.label.factorize()

print(sentiment_label)

"""Assign a number to each word in the sentences and replace each word with their respective assigned numbers.
Use word embeddings. This is capable of capturing the context of a word in a sentence or document.
"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
tweet = tweet_train_set.tweet.values
#the number of words to encode
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(tweet)
vocab_size = len(tokenizer.word_index) + 1
#To transforms each text in texts to a sequence of integers
encoded_docs = tokenizer.texts_to_sequences(tweet)
#To transforms a list of sequences (lists of integers) into a 2D Numpy array of shape (num_samples, num_timesteps)
#maxlen: maximum length of all sequences
padded_sequence = pad_sequences(encoded_docs, maxlen=200)

"""From the above code:
we get the actual texts from the data frame
Initialize the tokenizer with a 5000 word limit. This is the number of words we would like to encode.
we call fit_on_texts to create associations of words and numbers as shown in the image below.

# Part 1-1: **Building the Model**
Now that we have the inputs processed. It's time to build the model.
"""

### Part 1-1 - Building the model
# Build the model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.layers import SpatialDropout1D
from tensorflow.keras.layers import Embedding

embedding_vector_length = 32

model = Sequential()
#Input a 3D tensor with shape (samples, timesteps, channels)
model.add(Embedding(vocab_size, embedding_vector_length, input_length=200) )
model.add(SpatialDropout1D(0.25))
model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])

from keras.utils.vis_utils import plot_model
plot_model(model, to_file='model.png')
print(model.summary())

### Function - Timer : To calculate the ruuning time
import time

class TimerError(Exception):
    """A custom exception used to report errors in use of Timer class"""

class Timer:
    def __init__(self):
        self._start_time = None

    def start(self):
        """Start a new timer"""
        if self._start_time is not None:
            raise TimerError(f"Timer is running. Use .stop() to stop it")

        self._start_time = time.perf_counter()

    def stop(self):
        """Stop the timer, and report the elapsed time"""
        if self._start_time is None:
            raise TimerError(f"Timer is not running. Use .start() to start it")

        elapsed_time = time.perf_counter() - self._start_time
        self._start_time = None
        print(f"Elapsed time: {elapsed_time:0.4f} seconds")

t = Timer()

"""# Part 1-2: Training """

### Part 1-2 - Training the Model
t.start()
history = model.fit(padded_sequence,sentiment_label[0],
                  validation_split=0.2, epochs=5, batch_size=32)
t.stop()

#Checking for a single sentence 
test_word ="Trump is a fucking black president"
tw = tokenizer.texts_to_sequences([test_word])
tw = pad_sequences(tw,maxlen=200)
prediction = int(model.predict(tw).round().item())
sentiment_label[1][prediction]

"""# Part 2 : Testing"""

### Part3 - Testing

path2="/content/drive/MyDrive/Colab Notebooks/01 EDS_Assignments/P202 WDPS/asgn 02/Racism_Dataset/test.csv"
df2= pd.read_csv(path2, sep=',')
tweet_df2 = df2[['tweet']]

print(tweet_df2.iloc[4])

import numpy as np
res = pd.DataFrame(np.zeros(len(tweet_df2)))

### Function - To test a dataset 
def test_fn(test_word):
  #print(test_word)
  tw = tokenizer.texts_to_sequences([test_word])
  tw = pad_sequences(tw,maxlen=200)
  prediction = int(model.predict(tw).round().item())
  return sentiment_label[1][prediction]

t.start()
test_fn("Trump is a terrible black president")
t.stop()

#tweet_test_set
t.start()
res = tweet_test_set['tweet'].apply(test_fn)
t.stop()

test_n_result = pd.DataFrame(tweet_test_set['label'])
test_n_result ['test_result']= res
print(test_n_result)

test_n_result['SuccessCheck'] = np.where(test_n_result['label'] == test_n_result['test_result'], 1, 0)  #create new column in df1 to check if prices match
TrueAnswers = int(test_n_result.SuccessCheck.sum())
print("Number of True Answers",TrueAnswers)
no_rows = int(len(test_n_result))
print("Total number of lines", no_rows)
print("Success Rate")
rate = (TrueAnswers/no_rows)*100
print("{:.2f}".format(rate),'%')

res2 = df2['tweet'].apply(test_fn)

print(res2.shape)
print(res2)

print("Number of tweets: ")
print(len(tweet_df2))
print("Number of racist tweets: ")
res2.sum()

print("Number of tweets: ")
print(len(tweet_df2))
print("Number of racist tweets: ")
res2.sum()

"""# Part 3- Prediction """

#3#1# Getting all of the tweets datasets files
import glob 
print('List of Downloaded Dataset Tweets') 
DnTwPath="/content/drive/MyDrive/Colab Notebooks/01 EDS_Assignments/P202 WDPS/asgn 02/DownloadedTweets/"
lst=[]
for name in glob.glob(DnTwPath+'*.csv'): 
  print(name) 
  lst.append(name)
print("Number of Datasets: ",len(lst))

# Commented out IPython magic to ensure Python compatibility.
### Testing Function
def prd_fn(prd_word):
  tw = tokenizer.texts_to_sequences([prd_word])
  tw = pad_sequences(tw,maxlen=200)
  prediction = int(model.predict(tw).round().item())
  return sentiment_label[1][prediction]
# %time

### Cleaning Funtion 
import re
## version 2 of text cleaning
#set up punctuations we want to be replaced
REPLACE_NO_SPACE = re.compile("(\.)|(\;)|(\:)|(\!)|(\')|(\?)|(\,)|(\")|(\|)|(\()|(\))|(\[)|(\])|(\%)|(\$)|(\>)|(\<)|(\{)|(\})")
REPLACE_WITH_SPACE = re.compile("(<br\s/><br\s/?)|(-)|(/)|(:).")

# custum function to clean the dataset (combining tweet_preprocessor and regular expression)
def clean_tweets2(df):
    tempArr = []
    for line in df:
        # send to tweet_processor
        # tmpL = p.clean(line)
        tmpL=line
        # remove puctuation
        tmpL = REPLACE_NO_SPACE.sub("", tmpL.lower()) # convert all tweets to lower cases
        tmpL = REPLACE_WITH_SPACE.sub(" ", tmpL)
        if tmpL=='': ## try to eliminate empty string
            tmpL='XXX'
        tempArr.append(tmpL)
    return tempArr

#3#2# Reading every tweet of the dataset
import pandas as pd
path_test=lst[19]
print(path_test)
TweetSet= pd.read_csv(path_test, sep=',', names=['DateTime','tweet'], encoding='iso-8859-1')
print(len(TweetSet))
#3#3#clean training data
Test_TweetSet = clean_tweets2(TweetSet.tweet)
Test_TweetSet = pd.DataFrame(TweetSet)

#3#4# Prediction_testing 
t.start()
prd_res = Test_TweetSet['tweet'].apply(test_fn)
t.stop()

#3#5# Result of Prediction 
prdicted_result = int(prd_res.sum())
print("Number of Hated Tweets",prdicted_result)
total_tweets = int(len(prd_res))
print("Total Number of Tweets: ", total_tweets)
HS_rate = (prdicted_result/total_tweets)*100
print("{:.2f}".format(HS_rate),'%')